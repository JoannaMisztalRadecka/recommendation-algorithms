{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation algorithms in Python\n",
    "\n",
    "In this lab, we will implement different recommendation algorithms and evaluate their performance on a movie rating prediction task.\n",
    "\n",
    "**Task 1:** First, we will build a simple item-based kNN recommendation algorithm with different item feature representations. We will verify the outputs on a small dataset of movie ratings.\n",
    "\n",
    "**Task 2:** Next, we will use the `surprise` Python package with different collaborative filtering recommendation algorithm implementations and compare their performance for different hyperparametes setting on a standard movie ratings dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Item-based kNN recommendations with different item representations\n",
    "\n",
    "First, we wil implement an item-based approach to recommendations based on k Nearest Neighbors (kNN) model. We will evaluate it on the rating prediction task for a small sample of movies dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "We use a small version of the popular MovieLens movie recommendation dataset from GroupLens https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "urllib.request.urlretrieve(data_url, 'movielens.zip')\n",
    "movies_file = zipfile.ZipFile('movielens.zip')\n",
    "data_filename = 'ml-latest-small'\n",
    "\n",
    "ratings = pd.read_csv(movies_file.open('ml-latest-small/ratings.csv'))\n",
    "movies = pd.read_csv(movies_file.open('ml-latest-small/movies.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis \n",
    "First, we perform an exploratory analysis to learn basic characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the rating time distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['datetime'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "ratings['datetime'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the rating values distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.rating.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.rating.hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that each user has rated relatively few movies - the rating matrix is sparse which is a significant problem for the recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.groupby('movieId').count()['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.groupby('userId').count()['rating'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the user-item ratings\n",
    "We will implement an item-based k-nearest neighbors (kNN) recommendation algorithm. We will predict the user-movie ratings based on the user's ratings of the most similar movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split\n",
    "\n",
    "Usually in ML tasks, the train test split is performed at random. However, in real-world recommendation systems, the training is usually perfomed on historical data while the system needs to run in the following period of time. We will use the time split instead of random split for selecting the test set.\n",
    "\n",
    "In this task, we use only the movies and users that are available in both training and test data. Predicting the ratings of new users and items is known as the *cold-start problem* for recommendation systems and it's beyond the scope of this lab.\n",
    "\n",
    " <font color='red'>**ToDo:**</font>\n",
    "- Split the dataset to train and test sets by the rating time. Select the date to split such that the train/test split ratio is around 80/20%.\n",
    "\n",
    "    Hint: you may use `df[column].describe(percentiles=[...])` method to get the percentiles for the rating timestamp, docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ts = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = train_test_split(ratings)\n",
    "# we consider only the movies and users that are available in both training and test set \n",
    "# and only the movies with a record in metadata table.\n",
    "train_users = train_ratings.groupby('userId').count().reset_index()[['userId']]\n",
    "train_movies = train_ratings.groupby('movieId').count().reset_index()[['movieId']].merge(movies[['movieId']], on='movieId')\n",
    "movies = movies.merge(train_movies, on='movieId')\n",
    "test_ratings = ratings[ratings['timestamp'] > split_ts].merge(train_users, on='userId').merge(train_movies, on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ratings.count() / ratings.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a user-movie rating matrix\n",
    "First, we will construct the rating matrix $R$. Note that our rating matrix is sparse (there are many empty values) - as for now, we leave the empty values as NaNs. In this exercise we use a small dataset but for larger ones it is more efficient to use a sparse matrix instead of the dense one (we use a dense matrix due to better readability).\n",
    "\n",
    "<font color='red'>**ToDo:**</font>\n",
    "- Construct a pivot table with user ids as columns, movie ids as index and user-movie ratings as values. \n",
    "\n",
    "Hint: \n",
    "You may use `df.pivot_table(index=..., columns=..., values=...)` https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mx = ??\n",
    "rating_mx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the baseline predictions\n",
    "\n",
    "First, we need to define some baseline approaches for our recommendation algorithms and define the evaluation metric. We will use the  mean square error (MSE) between the true and predicted ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest baseline algorithm for rating prediction is an average rating for all movies.\n",
    "\n",
    "<font color='red'>**ToDo:**</font>\n",
    "- Prepare a new vector with predictions for each of the test ratings and fill it with mean value of all ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rating = ??\n",
    "pred_avg = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**ToDo:**</font>\n",
    "- Calculate `mean_squared_error` between the test set ratings and predictions: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_avg = ??\n",
    "print(mse_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-based kNN recommendations\n",
    "We will use the item-based kNN recommender - we will estimate the ratings for a new movie based on the ratings of similar movies in the user's history. We will define the similarity between movies for different movie representations.\n",
    "\n",
    "As the similarity measure, we use pairwise cosine similarity between movie feature vectors. This measure is more robust to sparse vectors than the Euclidean distance. We will calculate the similarity matrix for all movies pairs according to this metric.\n",
    "\n",
    "We define two help functions for retrieving the most similar movies to selected one based on the similarity matrix and predicting the user-item rating based on the user's ratings on the most simliar movies in their history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_movies(movie_id, similarity_df, n_neighbors=5):\n",
    "    similar_ids = similarity_df.loc[movie_id].sort_values(ascending=False)[1:n_neighbors+1].reset_index()\n",
    "    return similar_ids.merge(movies, on='movieId')[['title', 'genres']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_knn_rating(user_id, movie_id, movie_similarity, n_neighbors=5):\n",
    "    user_ratings = rating_mx[user_id].dropna()\n",
    "    neighbors_rated = movie_similarity.loc[user_ratings.index][movie_id].sort_values(ascending=False)[:n_neighbors]\n",
    "    predicted_rating = user_ratings.loc[neighbors_rated.index].mean()\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select an exemplary movie for a qualitative evaluation of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_movie = movies.iloc[0]\n",
    "test_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Content-Based Recommendations\n",
    "\n",
    "As the first approach, we use the content-based features of movies to calculate their similarity - in this case these are the movie genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'>**ToDo:**</font>\n",
    "- Use sklearn `CountVectorizer`  to build the content-based the movies features matrix from their genres https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html.\n",
    "- Split the lists of genres with | separator (use argument `token_pattern='[^|]+'` for `CountVectorizer`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = ??\n",
    "content_features = pd.DataFrame(vectorizer.fit_transform(movies['genres']).toarray(), columns=vectorizer.get_feature_names(), index=movies['movieId'])\n",
    "content_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'>**ToDo:**</font>\n",
    " - Build the similarity matrix for movies based on their `content_features`. \n",
    " \n",
    " Hint: use `cosine_similarity` function https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_content_mx = ??\n",
    "movie_similarity_content = pd.DataFrame(cosine_similarity_content_mx, columns=content_features.index, index=content_features.index)\n",
    "movie_similarity_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most similar movies to \"Toy Story\" based on the content features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_movies(test_movie['movieId'], movie_similarity_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the user-item kNN rating predictions based on the content features and calculate the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_content_based = test_ratings.apply(lambda x: get_item_knn_rating(x['userId'], x['movieId'], movie_similarity_content), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_knn_content = ??\n",
    "print(mse_knn_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Collaborative Filtering recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content-based approach is simple and quite effective (even in the new item situation) but it does not consider the information about the interaction patterns. To address this problem, we will implement a collaborative-filtering kNN recommendation algorithm which calculates the movies similarity based on the user rating matrix.\n",
    "\n",
    "<font color='red'>**ToDo:**</font>\n",
    "- As the ratings matrix is sparse, fill the empty values with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mx_filled = ??\n",
    "rating_mx_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_cf_mx = ??\n",
    "movie_similarity_cf = pd.DataFrame(cosine_similarity_cf_mx, columns=rating_mx_filled.index, index=rating_mx_filled.index)\n",
    "movie_similarity_cf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_movies(test_movie['movieId'], movie_similarity_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cf = test_ratings.apply(lambda x: get_item_knn_rating(x['userId'], x['movieId'], movie_similarity_cf), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_cf = ??\n",
    "print(mse_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Collaborative Filtering with Matrix Factorization\n",
    "\n",
    "One problem in the previous CF approach was the *curse of dimensionality* - the rating matrix was sparse and most of the items and users had very few ratings so the similarity was calculated with a limited information. One way to deal with this problem is to use the dimensionality reduction and find the latent factors in the user behavior. \n",
    "\n",
    "A popular method for latent features extraction in recommendation systems is matrix factorization. \n",
    "\n",
    " <font color='red'>**ToDo:**</font>\n",
    "- Use Non-negative Matrix Factorization `NMF` which assumes that the matrices have non-negative values from https://scikit-learn.org/stable/modules/decomposition.html#nmf\n",
    "- set parameter `n_components` to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mx_factors = pd.DataFrame(nmf.fit_transform(rating_mx_imputed), index=rating_mx_imputed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_mx_nmf = ??\n",
    "movie_similarity_factors = pd.DataFrame(similarity_mx_nmf, columns=rating_mx_factors.index, index=rating_mx_factors.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_movies(test_movie['movieId'], movie_similarity_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nmf = test_ratings.apply(lambda x: get_item_knn_rating(x['userId'], x['movieId'], movie_similarity_factors), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_nmf = ??\n",
    "print(mse_nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Comparing the performance of different recommendation algorithms \n",
    "In this task, we will use `surprise` python package to evaluate the performance of different recommendation algorithms on the movie recommendation task for a larger dataset.\n",
    "\n",
    "https://surprise.readthedocs.io/en/stable/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NMF, SVD, KNNBasic, KNNWithMeans, KNNWithZScore, NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV, cross_validate\n",
    "\n",
    "# use the 100 movie recommendation dataset.\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model \n",
    "\n",
    "<font color='red'>**ToDo:**</font>\n",
    "* Use the `NormalPredictor` which predicts a random rating based on the distribution of the training set (assumed to be normal) and evaluate it with 5-fold cross-validation.\n",
    "\n",
    "https://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.random_pred.NormalPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = ['rmse']\n",
    "results = {}\n",
    "\n",
    "baseline_normal = ??\n",
    "baseline_result = cross_validate(baseline_normal, data, measures=test_metrics)['test_rmse'].mean()\n",
    "results['baseline'] = baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN models\n",
    "First, tune and evaluate the KNN recommendation models. Read more about the algorithms: https://surprise.readthedocs.io/en/stable/knn_inspired.html\n",
    "\n",
    "<font color='red'>**ToDo:**</font>\n",
    "* Prepare the hyperparameters grid for the neighborhood-based models - `k` - number of neighbors from 10 to 100 (step 10) and boolean `user_based` for using user or item-based similarity.\n",
    "* Run `GridSearchCV` for the parameters grid for different KNN models - `KNNBasic`, `KNNWithMeans`, `KNNWithZScore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_neighbors = {'k': [??],\n",
    "                       'user_based': [??]}\n",
    "knn_models = [??]\n",
    "\n",
    "for knn_model in knn_models:\n",
    "    print('Evaluating model: {}'.format(knn_model))\n",
    "    search = GridSearchCV(knn_model, param_grid_neighbors, measures=test_metrics)\n",
    "    search.fit(data)\n",
    "    result = search.best_score['rmse']\n",
    "    results[knn_model.__name__] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization models\n",
    "Next, tune and evaluate the matrix factorization recommendation models. Read more about the algorithms: https://surprise.readthedocs.io/en/stable/matrix_factorization.html\n",
    "\n",
    "<font color='red'>**ToDo:**</font>\n",
    "* Prepare the hyperparameters grid for the MF models - `n_factors` - number of latent dimensions.\n",
    "* Run `GridSearchCV` for the parameters grid for MF models - `SVD`, `NMF`\n",
    "* save results for each model to results dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_mf = ??\n",
    "mf_models = [??]\n",
    "\n",
    "for mf_model in mf_models:\n",
    "    ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(results).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
